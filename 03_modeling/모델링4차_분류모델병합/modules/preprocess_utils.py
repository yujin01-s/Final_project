import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE 
from xgboost import XGBClassifier
from statsmodels.stats.outliers_influence import variance_inflation_factor

def load_and_process(file_path: str,
                     stage: str = None,
                     selected_cols: list = None,
                     base_cols: list = ["ID", "Segment"],
                     stage_feature_map: dict = None):
    """
    íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ê³ , ì„ íƒëœ ì»¬ëŸ¼ + base ì»¬ëŸ¼ë§Œ ìœ ì§€í•œ ë’¤ ë²”ì£¼í˜• ì¸ì½”ë”© ì²˜ë¦¬.

    1ì°¨: map_categorical_columns â†’ ë§¤í•‘
    2ì°¨: ë‚¨ì€ ë²”ì£¼í˜•ì— LabelEncoding ì ìš©

    Returns:
    - df_final (pd.DataFrame): ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„°í”„ë ˆì„
    - used_columns (list): ì‚¬ìš©ëœ ê°€ê³µ ëŒ€ìƒ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    """
    if stage is not None:
        if stage_feature_map is None:
            raise ValueError("stage_feature_mapì´ í•¨ìˆ˜ì— ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if stage not in stage_feature_map:
            raise ValueError(f"'{stage}'ëŠ” stage_feature_mapì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        selected_cols = stage_feature_map[stage]
        
    # 1. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    if file_path.endswith(".parquet"):
        df = pd.read_parquet(file_path)
    elif file_path.endswith(".csv"):
        df = pd.read_csv(file_path)
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. .parquet ë˜ëŠ” .csv")

    # 2. ì„ íƒëœ ì»¬ëŸ¼ + base_cols ê¸°ì¤€ í•„í„°ë§
    if selected_cols is not None:
        keep_cols = list(set(base_cols + selected_cols))
        df = df[keep_cols]

    # 3. ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
    df = df.loc[:, ~df.columns.duplicated()]

    # 4. ì‚¬ìš©ì ì •ì˜ ë§¤í•‘ í•¨ìˆ˜ ì ìš©
    df = map_categorical_columns(df)

    # 5. ì—¬ì „íˆ ë‚¨ì€ ë²”ì£¼í˜• ì»¬ëŸ¼(Label Encoding)
    object_cols = df.select_dtypes(include='object').columns.tolist()
    for col in object_cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        print(f"âœ… Label Encoding ì ìš©: {col}")

    # 6. ì‚¬ìš©ëœ ê°€ê³µ ëŒ€ìƒ ì»¬ëŸ¼ ê¸°ë¡ (base ì œì™¸)
    used_columns = [col for col in df.columns if col not in base_cols]

    return df, used_columns

# segment_target ì˜ˆì‹œ: ["C", "D"], ["A", "B"], ["E", "A", "B", "C", "D"]
def prepare_data(df, segment_target):
    X_all = df[df['Segment'].isin(segment_target)].copy()
    y_all = LabelEncoder().fit_transform(X_all['Segment'])
    return train_test_split(X_all, y_all, test_size=0.2, stratify=y_all, random_state=42)
    """
    # 1ë‹¨ê³„: E vs ë‚˜ë¨¸ì§€
    X_train, X_val, y_train, y_val = prepare_data(df, ['E', 'A', 'B', 'C', 'D'])

    # 2ë‹¨ê³„: AB vs CD
    X_train, X_val, y_train, y_val = prepare_data(df, ['A', 'B', 'C', 'D'])

    # 3ë‹¨ê³„: A vs B
    X_train, X_val, y_train, y_val = prepare_data(df, ['A', 'B'])

    # 4ë‹¨ê³„: C vs D
    X_train, X_val, y_train, y_val = prepare_data(df, ['C', 'D'])
    """

# VIF ê¸°ë°˜ í”¼ì²˜ ì œê±°
def remove_high_vif(df, features, fixed=[], threshold=10.0):
    features = features.copy()
    while True:
        X = df[features].fillna(0)
        vif = pd.Series(
            [variance_inflation_factor(X.values, i) for i in range(X.shape[1])],
            index=X.columns
        )
        max_vif = vif.drop(labels=fixed).max()
        if max_vif > threshold:
            to_remove = vif.drop(labels=fixed).idxmax()
            features.remove(to_remove)
        else:
            break
    return features

# ì˜ˆì¸¡ í™•ë¥  ìƒì„± í•¨ìˆ˜
def predict_proba_on_fixed_val(X_train, y_train, X_val, features):
    sm = SMOTE(random_state=42)
    X_train_res, y_train_res = sm.fit_resample(X_train[features], y_train)
    model = XGBClassifier(n_estimators=300, max_depth=4, learning_rate=0.1, random_state=42)
    model.fit(X_train_res, y_train_res)
    return model.predict_proba(X_val[features])[:, 1]

# ë¼ë²¨ì¸ì½”ë”©
def encode_categorical_columns(df):
    cat_cols = df.select_dtypes(include='object').columns.tolist()
    for col in cat_cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        print(f"ğŸ”µ {col} ì¸ì½”ë”© ì™„ë£Œ")
    return df

# X, y ë¶„ë¦¬ ë° ë²”ì£¼í˜•ì¸ì½”ë”© + ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (mean)
def seperateX_y(df, selected_feature):
    """
    í”¼ì²˜ ë° íƒ€ê²Ÿ ë¶„ë¦¬ + ë²”ì£¼í˜• ì¸ì½”ë”© + ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    Args:
        df: ì „ì²˜ë¦¬ ì™„ë£Œëœ DataFrame
        selected_feature: ì‚¬ìš©í•  í”¼ì²˜ ë¦¬ìŠ¤íŠ¸

    Returns:
        X: ì „ì²˜ë¦¬ ì™„ë£Œëœ í”¼ì²˜
        y: íƒ€ê²Ÿ
    """
    # 1. ë¶„ë¦¬
    X = df[selected_feature].copy()
    y = df["Segment"]

    # 2. ë²”ì£¼í˜• ì¸ì½”ë”©
    cat_cols = X.select_dtypes(include='object').columns.tolist()
    for col in cat_cols:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))

    # 3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    imputer = SimpleImputer(strategy='mean')
    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

    return X, y


# ë²”ì£¼í™” ì „ì²˜ë¦¬
def map_categorical_columns(df, verbose=True):
    """
    ë¯¸ë¦¬ ì •ì˜ëœ ë§¤í•‘ ê¸°ì¤€ì— ë”°ë¼ ë²”ì£¼í˜• ì»¬ëŸ¼ë“¤ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì²˜ë¦¬ ì»¬ëŸ¼: ê±°ì£¼ì‹œë„ëª…, ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M, í•œë„ì¦ì•¡íšŸìˆ˜_R12M, ì´ìš©ê¸ˆì•¡ëŒ€,
              í• ì¸ê±´ìˆ˜_R3M, í• ì¸ê±´ìˆ˜_B0M, ë°©ë¬¸íšŸìˆ˜_PC_R6M, ë°©ë¬¸íšŸìˆ˜_ì•±_R6M, ë°©ë¬¸ì¼ìˆ˜_PC_R6M
    """

    # 1. ê±°ì£¼ì‹œë„ëª… â†’ ìˆ˜ë„ê¶Œ ì—¬ë¶€
    capital_area = ['ì„œìš¸', 'ê²½ê¸°', 'ì¸ì²œ']
    if 'ê±°ì£¼ì‹œë„ëª…' in df.columns:
        df['ê±°ì£¼ì‹œë„ëª…'] = df['ê±°ì£¼ì‹œë„ëª…'].apply(lambda x: 1 if x in capital_area else 0)

    # 2. ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M
    mapping = {"0ê°œ": 0, "1ê°œì´ìƒ": 1}
    if 'ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M' in df.columns:
        df['ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M'] = df['ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M'].map(mapping).astype(int)
        if verbose: print("[ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M] ì¸ì½”ë”© ì™„ë£Œ")

    # 3. í•œë„ì¦ì•¡íšŸìˆ˜_R12M
    mapping = {"0íšŒ": 0, "1íšŒì´ìƒ": 1}
    if 'í•œë„ì¦ì•¡íšŸìˆ˜_R12M' in df.columns:
        df['í•œë„ì¦ì•¡íšŸìˆ˜_R12M'] = df['í•œë„ì¦ì•¡íšŸìˆ˜_R12M'].map(mapping).astype(int)
        if verbose: print("[í•œë„ì¦ì•¡íšŸìˆ˜_R12M] ì¸ì½”ë”© ì™„ë£Œ")

    # 4. ì´ìš©ê¸ˆì•¡ëŒ€ (ì¤‘ê°„ê°’ ê¸°ì¤€: ë§Œì› ë‹¨ìœ„)
    mapping = {
        "09.ë¯¸ì‚¬ìš©": 0,
        "05.10ë§Œì›-": 5,
        "04.10ë§Œì›+": 20,
        "03.30ë§Œì›+": 40,
        "02.50ë§Œì›+": 75,
        "01.100ë§Œì›+": 150
    }
    if 'ì´ìš©ê¸ˆì•¡ëŒ€' in df.columns:
        df['ì´ìš©ê¸ˆì•¡ëŒ€'] = df['ì´ìš©ê¸ˆì•¡ëŒ€'].map(mapping)
        if verbose: print("[ì´ìš©ê¸ˆì•¡ëŒ€] ì¤‘ê°„ê°’ ì¸ì½”ë”© ì™„ë£Œ")

   # 5. í• ì¸ê±´ìˆ˜ ì¸ì½”ë”©
    discount_map = {
        "1íšŒ ì´ìƒ": 1,
        "10íšŒ ì´ìƒ": 10,
        "20íšŒ ì´ìƒ": 20,
        "30íšŒ ì´ìƒ": 30,
        "40íšŒ ì´ìƒ": 40
    }
    for col in ['í• ì¸ê±´ìˆ˜_R3M', 'í• ì¸ê±´ìˆ˜_B0M']:
        if col in df.columns:
            df[col] = df[col].map(discount_map).astype(int)
            if verbose: print(f"[{col}] ì¸ì½”ë”© ì™„ë£Œ")

    # 6. ë°©ë¬¸íšŸìˆ˜ ë° ë°©ë¬¸ì¼ìˆ˜ ì¸ì½”ë”©
    visit_map = {
        "1íšŒ ì´ìƒ": 1,
        "10íšŒ ì´ìƒ": 10,
        "20íšŒ ì´ìƒ": 20,
        "30íšŒ ì´ìƒ": 30,
        "40íšŒ ì´ìƒ": 40,
        "50íšŒ ì´ìƒ": 50,
        "60íšŒ ì´ìƒ": 60,
        "70íšŒ ì´ìƒ": 70,
        "80íšŒ ì´ìƒ": 80
    }

    visit_cols = ['ë°©ë¬¸íšŸìˆ˜_PC_R6M', 'ë°©ë¬¸íšŸìˆ˜_ì•±_R6M', 'ë°©ë¬¸ì¼ìˆ˜_PC_R6M']
    for col in visit_cols:
        if col in df.columns:
            df[col] = df[col].map(visit_map).astype(int)
            if verbose: print(f"[{col}] ì¸ì½”ë”© ì™„ë£Œ")

    return df

import pandas as pd
import numpy as np

def get_high_correlation_pairs(df, threshold=0.8):
    """
    ìƒê´€ê³„ìˆ˜ ì ˆëŒ“ê°’ì´ threshold ì´ìƒì¸ ë³€ìˆ˜ìŒì„ í‘œë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ìê¸° ìì‹  ì œì™¸, ì¤‘ë³µ ì œê±°)
    """
    # ìˆ˜ì¹˜í˜•ë§Œ ì„ íƒ
    corr = df.select_dtypes(include=['int64', 'float64']).corr()

    # ìƒê´€ê³„ìˆ˜ í–‰ë ¬ì„ long-formatìœ¼ë¡œ ë³€í™˜
    corr_pairs = corr.unstack().reset_index()
    corr_pairs.columns = ['Feature_1', 'Feature_2', 'Correlation']

    # ìê¸° ìì‹ ì€ ì œì™¸
    corr_pairs = corr_pairs[corr_pairs['Feature_1'] != corr_pairs['Feature_2']]

    # ì¤‘ë³µ ì œê±° (ì˜ˆ: A-Bì™€ B-A ì¤‘ í•˜ë‚˜ë§Œ)
    corr_pairs['sorted'] = corr_pairs.apply(lambda row: tuple(sorted([row['Feature_1'], row['Feature_2']])), axis=1)
    corr_pairs = corr_pairs.drop_duplicates(subset='sorted').drop(columns='sorted')

    # ì ˆëŒ“ê°’ ê¸°ì¤€ ì •ë ¬ ë° í•„í„°ë§
    corr_pairs['AbsCorr'] = corr_pairs['Correlation'].abs()
    high_corr = corr_pairs[corr_pairs['AbsCorr'] >= threshold].sort_values(by='AbsCorr', ascending=False)

    return high_corr[['Feature_1', 'Feature_2', 'Correlation']]

from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler
import pandas as pd

def calculate_vif(df, exclude_cols=['ID', 'Segment']):
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì„ íƒ
    X = df.select_dtypes(include=['int64', 'float64']).drop(columns=exclude_cols, errors='ignore')

    # ê²°ì¸¡ì¹˜ ì œê±° ë˜ëŠ” ì„ì‹œ ì±„ìš°ê¸° (VIF ê³„ì‚°ì€ ê²°ì¸¡ í—ˆìš© ì•ˆë¨)
    X = X.fillna(0)

    # ì •ê·œí™” (StandardScaler)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # VIF ê³„ì‚°
    vif_df = pd.DataFrame()
    vif_df["feature"] = X.columns
    vif_df["VIF"] = [variance_inflation_factor(X_scaled, i) for i in range(X.shape[1])]

    # ì •ë ¬
    return vif_df.sort_values(by="VIF", ascending=False)

from sklearn.linear_model import LinearRegression
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

def fast_vif(X, verbose=True):
    X = X.copy()
    X = X.fillna(0)  # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    X = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)

    vif_dict = {}
    for i in range(X.shape[1]):
        y = X.iloc[:, i]
        X_not_i = X.drop(X.columns[i], axis=1)
        model = LinearRegression().fit(X_not_i, y)
        r2 = model.score(X_not_i, y)
        vif = 1 / (1 - r2) if r2 < 1 else np.inf
        vif_dict[X.columns[i]] = vif
        if verbose:
            print(f"{X.columns[i]}: VIF={vif:.2f}")

    vif_df = pd.DataFrame(vif_dict.items(), columns=["feature", "VIF"]).sort_values(by="VIF", ascending=False)
    return vif_df

from modules.feature_selector import safe_div

def auto_reduce_vif_features_fast(df, high_corr_table, vif_threshold=10.0, corr_threshold=0.9, top_k=20, verbose=True):
    """
    VIF ë†’ì€ ì»¬ëŸ¼ ê¸°ë°˜ ê³ ìƒê´€ ë³€ìˆ˜ìŒ â†’ ì°¨ì´/ë¹„ìœ¨ íŒŒìƒë³€ìˆ˜ ìƒì„± (Top-K)
    + ìƒìœ„ 30ê°œ VIF ì¶œë ¥ í¬í•¨
    """
    df = df.copy()

    # 1. ê³ ìƒê´€ í”¼ì²˜ìŒ ë¨¼ì € í•„í„°ë§
    filtered_corr = high_corr_table[high_corr_table["Correlation"].abs() > corr_threshold]

    # 2. í•´ë‹¹ ì»¬ëŸ¼ë§Œìœ¼ë¡œ VIF ê³„ì‚°
    candidate_cols = set(filtered_corr["Feature_1"]) | set(filtered_corr["Feature_2"])
    candidate_cols = [col for col in candidate_cols if col in df.columns]
    vif_df = fast_vif(df[candidate_cols], verbose=False)

    # 3. ìƒìœ„ 30ê°œ VIF ì¶œë ¥
    top30_vif = vif_df.sort_values(by="VIF", ascending=False).head(30)
    print("\nğŸ“Š [VIF ìƒìœ„ 30ê°œ]")
    print(top30_vif.to_string(index=False))

    # 4. VIF ë†’ì€ ë³€ìˆ˜ í•„í„°
    high_vif_cols = set(vif_df[vif_df["VIF"] > vif_threshold]["feature"])

    # 5. ê³ ìƒê´€ + VIF ë†’ì€ ìŒë§Œ Top-K
    high_corr_filtered = filtered_corr[
        (filtered_corr["Feature_1"].isin(high_vif_cols)) |
        (filtered_corr["Feature_2"].isin(high_vif_cols))
    ].sort_values(by="Correlation", ascending=False).head(top_k)

    # 6. íŒŒìƒë³€ìˆ˜ ìƒì„±
    created_features = []
    for _, row in high_corr_filtered.iterrows():
        f1, f2 = row["Feature_1"], row["Feature_2"]
        if f1 in df.columns and f2 in df.columns:
            diff_col = f"{f1}_minus_{f2}"
            ratio_col = f"{f1}_div_{f2}"

            df[diff_col] = df[f1] - df[f2]
            df[ratio_col] = safe_div(df[f1], df[f2] + 1e-5)
            created_features.extend([diff_col, ratio_col])

            if verbose:
                print(f"âœ… Created: {diff_col}, {ratio_col}")

    return df, created_features, top30_vif

def remove_inf_div_features_fast(df, only_div_cols=True, verbose=True, threshold=0.01):
    """
    ë¬´í•œê°’(inf) ë˜ëŠ” NaN ë¹„ì¤‘ì´ ë†’ì€ ì»¬ëŸ¼ ì œê±°
    - only_div_cols=True: '_div_'ê°€ í¬í•¨ëœ ì»¬ëŸ¼ë§Œ ê²€ì‚¬
    """
    df = df.copy()
    cols_to_remove = []

    target_cols = df.columns
    if only_div_cols:
        target_cols = [col for col in df.columns if '_div_' in col]

    for col in target_cols:
        if df[col].dtype not in ['float64', 'int64']:
            continue

        inf_ratio = np.isinf(df[col]).mean()
        nan_ratio = df[col].isna().mean()

        if inf_ratio > threshold or nan_ratio > threshold:
            cols_to_remove.append(col)
            if verbose:
                print(f"ğŸš« ì œê±°ë¨: {col} (inf_ratio={inf_ratio:.4f}, nan_ratio={nan_ratio:.4f})")

    df.drop(columns=cols_to_remove, inplace=True)
    return df, cols_to_remove

def auto_clean_high_vif_features(df, verbose=True):
    """
    VIF ìƒìœ„ í”¼ì²˜ ìë™ ì •ë¦¬:
    - ì‹ ìš©/ì‹ íŒ/ì¼ì‹œë¶ˆ: í•˜ë‚˜ë§Œ ë‚¨ê¹€
    - í•œë„ì†Œì§„ìœ¨ ê³„ì—´: ìƒìœ„ 2ê°œ ì œì™¸í•˜ê³  ì°¨ì´/ë¹„ìœ¨ íŒŒìƒ
    - ì¹´ë“œë¡  B0M~B2M: ë³€í™”ëŸ‰/ë¹„ìœ¨ íŒŒìƒ í›„ ì¼ë¶€ ì œê±°
    - ë°©ë¬¸ì¼ìˆ˜/íšŸìˆ˜: íŒŒìƒ ìƒì„± í›„ ì œê±°
    - í¬ì¸íŠ¸ ê±´ë³„: ì°¨ì´ íŒŒìƒ í›„ ì œê±°
    """
    df = df.copy()
    removed_cols = []
    created_cols = []

    # 1. ì‹ ìš©/ì‹ íŒ/ì¼ì‹œë¶ˆ ì¤‘ 'ì‹ ìš©'ë§Œ ìœ ì§€
    candidates = ['ì´ìš©ê±´ìˆ˜_ì‹ íŒ_R12M', 'ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_R12M']
    for col in candidates:
        if col in df.columns:
            df.drop(columns=col, inplace=True)
            removed_cols.append(col)
            if verbose: print(f"ğŸš« ì œê±°: {col}")

    # 2. í•œë„ì†Œì§„ìœ¨ ê³„ì—´
    hando_cols = [col for col in df.columns if 'í•œë„ì†Œì§„ìœ¨' in col]
    hando_sorted = df[hando_cols].std().sort_values(ascending=False).index[:2].tolist()  # ìƒìœ„ 2ê°œ ìœ ì§€
    for col in hando_cols:
        if col not in hando_sorted:
            # íŒŒìƒ: ì°¨ì´ + ë¹„ìœ¨
            df[f"{hando_sorted[0]}_minus_{col}"] = df[hando_sorted[0]] - df[col]
            df[f"{hando_sorted[0]}_div_{col}"] = safe_div(df[hando_sorted[0]], df[col] + 1e-5)
            created_cols += [f"{hando_sorted[0]}_minus_{col}", f"{hando_sorted[0]}_div_{col}"]
            df.drop(columns=col, inplace=True)
            removed_cols.append(col)
            if verbose: print(f"ğŸš« ì œê±° + íŒŒìƒ ìƒì„±: {col}")

    # 3. ì¹´ë“œë¡  ì‹œê³„ì—´: B0M, B1M, B2M â†’ B0M ìœ ì§€
    card_cols = ['ì”ì•¡_ì¹´ë“œë¡ _B0M', 'ì”ì•¡_ì¹´ë“œë¡ _B1M', 'ì”ì•¡_ì¹´ë“œë¡ _B2M']
    if all(col in df.columns for col in card_cols):
        df['ì¹´ë“œë¡ _ë³€í™”ëŸ‰'] = df['ì”ì•¡_ì¹´ë“œë¡ _B0M'] - df['ì”ì•¡_ì¹´ë“œë¡ _B2M']
        df['ì¹´ë“œë¡ _ë¹„ìœ¨'] = safe_div(df['ì”ì•¡_ì¹´ë“œë¡ _B0M'], df['ì”ì•¡_ì¹´ë“œë¡ _B2M'] + 1e-5)
        created_cols += ['ì¹´ë“œë¡ _ë³€í™”ëŸ‰', 'ì¹´ë“œë¡ _ë¹„ìœ¨']
        for col in ['ì”ì•¡_ì¹´ë“œë¡ _B1M', 'ì”ì•¡_ì¹´ë“œë¡ _B2M']:
            df.drop(columns=col, inplace=True)
            removed_cols.append(col)
            if verbose: print(f"ğŸš« ì œê±°: {col} (ì¹´ë“œë¡  ì‹œê³„ì—´)")

    # 4. ë°©ë¬¸íšŸìˆ˜/ì¼ìˆ˜ ì•±
    if 'ë°©ë¬¸íšŸìˆ˜_ì•±_B0M' in df.columns and 'ë°©ë¬¸ì¼ìˆ˜_ì•±_B0M' in df.columns:
        df['ë°©ë¬¸ë¹ˆë„_ì•±'] = safe_div(df['ë°©ë¬¸íšŸìˆ˜_ì•±_B0M'], df['ë°©ë¬¸ì¼ìˆ˜_ì•±_B0M'] + 1)
        created_cols.append('ë°©ë¬¸ë¹ˆë„_ì•±')
        df.drop(columns=['ë°©ë¬¸ì¼ìˆ˜_ì•±_B0M'], inplace=True)
        removed_cols.append('ë°©ë¬¸ì¼ìˆ˜_ì•±_B0M')
        if verbose: print("âœ… ìƒì„±: ë°©ë¬¸ë¹ˆë„_ì•±, ì œê±°: ë°©ë¬¸ì¼ìˆ˜_ì•±_B0M")

    # 5. í¬ì¸íŠ¸ ê±´ë³„
    if 'í¬ì¸íŠ¸_ë§ˆì¼ë¦¬ì§€_ê±´ë³„_R3M' in df.columns and 'í¬ì¸íŠ¸_ë§ˆì¼ë¦¬ì§€_ê±´ë³„_B0M' in df.columns:
        df['í¬ì¸íŠ¸ê±´ë³„_ë³€í™”ëŸ‰'] = df['í¬ì¸íŠ¸_ë§ˆì¼ë¦¬ì§€_ê±´ë³„_R3M'] - df['í¬ì¸íŠ¸_ë§ˆì¼ë¦¬ì§€_ê±´ë³„_B0M']
        created_cols.append('í¬ì¸íŠ¸ê±´ë³„_ë³€í™”ëŸ‰')
        df.drop(columns=['í¬ì¸íŠ¸_ë§ˆì¼ë¦¬ì§€_ê±´ë³„_B0M'], inplace=True)
        removed_cols.append('í¬ì¸íŠ¸_ë§ˆì¼ë¦¬ì§€_ê±´ë³„_B0M')
        if verbose: print("âœ… ìƒì„±: í¬ì¸íŠ¸ê±´ë³„_ë³€í™”ëŸ‰, ì œê±°: í¬ì¸íŠ¸_ë§ˆì¼ë¦¬ì§€_ê±´ë³„_B0M")

    return df, removed_cols, created_cols

def remove_high_vif_features(df, vif_threshold=60.0, verbose=True):
    """
    VIF ê³„ì‚° í›„ thresholdë³´ë‹¤ ë†’ì€ ì»¬ëŸ¼ ìë™ ì œê±°
    ë°˜í™˜ê°’: ì •ì œëœ DataFrame, ì œê±°ëœ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸, ìµœì¢… VIF DataFrame
    """
    df = df.copy()
    removed_cols = []

    while True:
        vif_df = fast_vif(df, verbose=False)
        max_vif = vif_df["VIF"].max()

        if max_vif <= vif_threshold:
            break  # ëª¨ë‘ threshold ì´í•˜ë©´ ì¢…ë£Œ

        # ì œê±°í•  ì»¬ëŸ¼ (VIF ê°€ì¥ ë†’ì€ ë³€ìˆ˜ 1ê°œ)
        remove_col = vif_df.iloc[0]["feature"]
        df.drop(columns=remove_col, inplace=True)
        removed_cols.append(remove_col)

        if verbose:
            print(f"ğŸš« ì œê±°: {remove_col} (VIF={max_vif:.2f})")

    # ìµœì¢… VIF ê²°ê³¼ ë¦¬í„´
    final_vif_df = fast_vif(df, verbose=False)
    return df, removed_cols, final_vif_df

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

def fast_vif_cleaner(X, vif_threshold=100.0, remove_high_vif=True, verbose=True):
    """
    âœ… fast_vif_cleaner: ë¹ ë¥¸ VIF ê³„ì‚° ë° ê³ ë‹¤ì¤‘ê³µì„ ì„± ë³€ìˆ˜ ì œê±° í•¨ìˆ˜ (ëª¨ë“ˆìš©)
    
    Parameters:
    - X (pd.DataFrame): ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ í¬í•¨ëœ ì…ë ¥ ë°ì´í„°í”„ë ˆì„
    - vif_threshold (float): VIF ì„ê³„ê°’ (default=100.0)
    - remove_high_vif (bool): VIF ì´ˆê³¼ ë³€ìˆ˜ ë°˜ë³µ ì œê±° ì—¬ë¶€
    - verbose (bool): ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
    - X_cleaned (pd.DataFrame): ì •ì œëœ ë³€ìˆ˜ ë°ì´í„°í”„ë ˆì„
    - removed_cols (list): ì œê±°ëœ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
    - final_vif (pd.DataFrame): ìµœì¢… VIF ê°’ DataFrame
    """

    X = X.copy()
    X.replace([np.inf, -np.inf], np.nan, inplace=True)
    X.dropna(axis=1, inplace=True)
    X = X.fillna(0)

    scaler = StandardScaler()
    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

    removed_cols = []

    def compute_vif(df):
        vif_dict = {}
        for i in range(df.shape[1]):
            y = df.iloc[:, i]
            X_not_i = df.drop(df.columns[i], axis=1)
            model = LinearRegression().fit(X_not_i, y)
            r2 = model.score(X_not_i, y)
            vif = 1 / (1 - r2) if r2 < 0.9999 else np.inf
            vif_dict[df.columns[i]] = vif
            if verbose:
                print(f"ğŸ“Œ {df.columns[i]}: VIF={vif:.2f}")
        return pd.DataFrame(vif_dict.items(), columns=["feature", "VIF"]).sort_values(by="VIF", ascending=False)

    while True:
        vif_df = compute_vif(X_scaled)
        max_vif = vif_df["VIF"].max()

        if remove_high_vif and max_vif > vif_threshold:
            to_remove = vif_df.iloc[0]["feature"]
            if verbose:
                print(f"ğŸš« '{to_remove}' ì œê±° (VIF={max_vif:.2f})")
            X_scaled.drop(columns=[to_remove], inplace=True)
            removed_cols.append(to_remove)
        else:
            break

    final_vif = compute_vif(X_scaled)

    return X_scaled, removed_cols, final_vif

def get_clean_numeric_columns(df, columns):
    """
    ìˆ«ìí˜•ì´ê³  1ì°¨ì›ì¸ ì»¬ëŸ¼ë§Œ í•„í„°ë§
    """
    clean_cols = []
    for col in columns:
        try:
            if pd.api.types.is_numeric_dtype(df[col]) and df[col].ndim == 1:
                clean_cols.append(col)
            else:
                print(f"âŒ ì œì™¸ë¨: '{col}' (ndim={df[col].ndim}, dtype={df[col].dtypes})")
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {col}, error = {e}")
    return clean_cols